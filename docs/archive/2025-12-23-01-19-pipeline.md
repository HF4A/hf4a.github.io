# Scan Pipeline Documentation

**Created:** 2025-12-23 01:19am
**Purpose:** Detailed step-by-step breakdown of what happens when the SCAN button is pressed

---

## Overview

When the user presses SCAN, the app captures a still frame from the camera, detects card-shaped quadrilaterals using OpenCV, extracts each card region, computes perceptual hashes, and matches against a pre-built index. The full scan (image + card metadata) is stored in browser localStorage as a base64 data URL.

---

## Phase 1: Frame Capture

**Location:** `ShowxatingShell.tsx` → `capture()` function (lines 39-170)

### Step 1.1: Video Element Access
```
cameraViewRef.current?.videoRef?.current
```
- Gets reference to the live `<video>` element displaying camera feed
- Validates video is ready: `videoWidth > 0` and `videoHeight > 0`
- If not ready, capture is aborted

### Step 1.2: Canvas Creation
- Creates (or reuses) an offscreen `<canvas>` element
- Canvas dimensions set to match video exactly:
  - **Typical mobile:** 1920x1080 or 1280x720 (depends on device camera)
  - **No downscaling** at this stage - full resolution capture

### Step 1.3: Frame Render
```javascript
ctx.drawImage(video, 0, 0)
```
- Draws current video frame onto canvas
- This is a synchronous operation - captures exact moment SCAN was pressed

### Step 1.4: Base64 Conversion
```javascript
const imageDataUrl = canvas.toDataURL('image/jpeg', 0.85)
```
- Converts canvas to JPEG data URL
- **Quality:** 85% (balances file size vs. quality)
- **Format:** `data:image/jpeg;base64,/9j/4AAQ...`
- **Typical size:** 150-400KB for a 1920x1080 frame
- This data URL is stored in localStorage later
- #ross-note - we should try with higher quality

### Step 1.5: ImageData Extraction
```javascript
const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height)
```
- Extracts raw RGBA pixel data for OpenCV processing
- **Format:** `ImageData { width, height, data: Uint8ClampedArray }`
- **Size:** width × height × 4 bytes (e.g., 1920×1080×4 = ~8.3MB in memory)
- This is temporary - used for detection, then garbage collected

---

## Phase 2: Card Detection (OpenCV)

**Location:** `visionPipeline.ts` → `detectAllCards()` function

### Step 2.1: OpenCV Mat Creation
```javascript
src = cv.matFromImageData(imageData)
```
- Converts ImageData to OpenCV Mat format
- Full resolution - no downscaling

### Step 2.2: Grayscale Conversion
```javascript
cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY)
```
- Converts RGBA to single-channel grayscale
- Reduces data from 4 channels to 1

### Step 2.3: Gaussian Blur
```javascript
cv.GaussianBlur(gray, blurred, new cv.Size(5, 5), 0)
```
- 5×5 kernel blur to reduce noise
- Helps Canny edge detection find cleaner edges

### Step 2.4: Canny Edge Detection
```javascript
cv.Canny(blurred, edges, 50, 150)
```
- Low threshold: 50
- High threshold: 150
- Produces binary edge map

### Step 2.5: Contour Finding
```javascript
cv.findContours(edges, contours, hierarchy, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
```
- Finds external contours only (RETR_EXTERNAL)
- Simplifies contours to essential points (CHAIN_APPROX_SIMPLE)

### Step 2.6: Quadrilateral Filtering

For each contour:

1. **Area Filter:**
   - Minimum: 2% of frame area
   - Maximum: 60% of frame area
   - A 1920×1080 frame = 2,073,600 pixels
   - Min card area: ~41,472 pixels (roughly 200×200)
   - Max card area: ~1,244,160 pixels

2. **Polygon Approximation:**
   ```javascript
   cv.approxPolyDP(contour, approx, epsilon, true)
   ```
   - Epsilon = 4% of contour perimeter
   - Simplifies contour to polygon

3. **Quadrilateral Check:**
   - Must have exactly 4 corners (`approx.rows === 4`)
   - Must be convex (`cv.isContourConvex(approx)`)
   - #ross-note - why must be convex?

4. **Aspect Ratio Check:**
   - Expected card ratio: 88mm / 63mm = 1.4 (height/width)
   - Tolerance: ±0.35
   - Also accepts inverted ratio (landscape orientation)

### Step 2.7: Corner Ordering
- Corners are sorted clockwise starting from top-left
- Order: TL → TR → BR → BL
- Based on angle from centroid, then finding smallest x+y sum for TL

### Step 2.8: Size Consistency Filter
- Calculates median area of all detected cards
- Filters out cards that differ from median by more than 40%
- Ensures detected cards are roughly same size (same distance from camera)

**Output:** Array of `DetectionResult` objects, each containing:
- `corners`: 4 points (TL, TR, BR, BL)
- `area`: pixel area
- `aspectRatio`: height/width
- `confidence`: 0-1 score

---

## Phase 3: Perspective Warp

**Location:** `visionPipeline.ts` → `warpCardToRectangle()` function

For each detected card:

### Step 3.1: Source Points
- The 4 detected corners from Phase 2

### Step 3.2: Destination Points
- A perfect rectangle:
  - **Width:** 200 pixels
  - **Height:** 280 pixels
  - Based on card aspect ratio (63mm × 88mm)

### Step 3.3: Perspective Transform
```javascript
transform = cv.getPerspectiveTransform(srcPoints, dstPoints)
cv.warpPerspective(src, dst, transform, dsize)
```
- Computes 3×3 transformation matrix
- Warps the detected quadrilateral region to a flat 200×280 rectangle
- Corrects for camera angle and perspective distortion

### Step 3.4: Output Canvas
- Creates new canvas: **200×280 pixels**
- Renders warped card image to this canvas
- This standardized size is used for hash computation

---

## Phase 4: Type Detection

**Location:** `cardMatcher.ts` → `detectCardTypeFromCanvas()` function

Before hash matching, attempts to detect card type from color analysis:

### Step 4.1: Icon Region Extraction
- Analyzes top-left 25% width × 15% height of card
- This is where type icons appear on HF4A cards

### Step 4.2: Color Analysis
- Converts each pixel to HSL
- Skips very dark (L < 0.1) or very light (L > 0.95) pixels
- Builds hue histogram (36 bins, 10° each)
- Finds dominant hue

### Step 4.3: Type Matching
Compares dominant hue/saturation against known signatures:
- **Thruster:** Yellow-orange (hue 30-60°)
- **Reactor:** Red-orange (hue 0-30°)
- **Radiator:** Blue-cyan (hue 180-220°)
- **Robonaut:** Gray/metallic (low saturation)
- **Refinery:** Brown-orange (hue 15-45°)
- etc.

**Output:** Array of likely card types (up to 5), most likely first

---

## Phase 5: Hash Computation (dHash)

**Location:** `cardMatcher.ts` → `computeDHashFromImageData()` function

### Step 5.1: Resize to 9×8
```javascript
tempCtx.drawImage(canvas, srcX, srcY, srcW, srcH, 0, 0, 9, 8)
```
- Takes the 200×280 warped card image
- Resizes to **9 pixels wide × 8 pixels tall**
- Browser's bilinear interpolation averages the pixels

### Step 5.2: Grayscale Conversion
```javascript
gray = 0.299*R + 0.587*G + 0.114*B
```
- Standard luminance formula
- Produces 72 grayscale values (9×8)

### Step 5.3: Difference Hash Computation
For each row (8 rows):
  For each column (8 comparisons):
    - Compare pixel[x] to pixel[x+1]
    - If left > right: bit = 1
    - Else: bit = 0

- Produces **64 bits** total (8×8)
- Stored as **8 bytes**

### Step 5.4: Hex Representation
```javascript
hashBytes.map(b => b.toString(16).padStart(2, '0')).join('')
```
- Converts to 16-character hex string
- Example: `"71613ee3c66a700d"`

---

## Phase 6: Index Matching

**Location:** `cardMatcher.ts` → `matchFromHashWithDebug()` function

### Step 6.1: Load Card Index
- Pre-computed hashes loaded from `public/data/card-index.json`
- **358 entries** (as of current build)
- Each entry: `{ filename, cardId, side, hash, hashBytes }`

### Step 6.2: Active Type Filtering
- Only matches against card types enabled in settings
- Default: all types active

### Step 6.3: Hamming Distance Calculation
```javascript
function hammingDistance(hash1, hash2) {
  let distance = 0;
  for (let i = 0; i < 8; i++) {
    let xor = hash1[i] ^ hash2[i];
    while (xor) {
      distance += xor & 1;
      xor >>= 1;
    }
  }
  return distance;
}
```
- XORs each byte pair
- Counts differing bits
- **Range:** 0 (identical) to 64 (completely different)

### Step 6.4: Match Thresholds
- **Maximum distance for match:** 22 bits (~34% different)
- **Confident match:** ≤14 bits (~22% different)
- Type-filtered cards get stricter threshold: 17 bits for non-matching types

### Step 6.5: Confidence Scoring
```javascript
confidence = distance <= 14
  ? 1.0 - (distance / 14) * 0.3
  : 0.7 - ((distance - 14) / 8) * 0.4
```
- Plus 0.15 boost if detected type matches

### Step 6.6: Result Sorting
- Primary: Type match preference
- Secondary: Hamming distance (lowest first)

**Output:** `MatchResult[]` with:
- `cardId`: e.g., `"refinery-04"`
- `filename`: e.g., `"Refinery04-White-CarboChlorination.webp"`
- `side`: `"white"`, `"black"`, `"purple"`, etc.
- `distance`: Hamming distance
- `confidence`: 0-1 score

---

## Phase 7: Storage

**Location:** `showxatingStore.ts`

### Step 7.1: Scan Record Creation
```javascript
const scan: CapturedScan = {
  id: `scan-${Date.now()}`,
  timestamp: Date.now(),
  imageDataUrl,  // Full base64 JPEG (~150-400KB)
  cards: identifiedCards  // Array of IdentifiedCard objects
}
```

### Step 7.2: Slot Management
- 7 slots available: s1 through s7
- New scans go to s1, others shift down
- s7 falls off (discarded)

### Step 7.3: LocalStorage Persistence
```javascript
useScanSlotsStore.setState({ scanSlots: newSlots })
```
- Uses Zustand's `persist` middleware
- **Storage key:** `"showxating-scan-slots"`
- **Stored as:** JSON-serialized object
- **Contains:** Full base64 images for all 7 slots

### Step 7.4: Storage Size Implications
- Each scan: ~150-400KB base64 image + ~1KB metadata
- Maximum 7 slots: ~1-3MB total localStorage usage
- Browser localStorage limit: typically 5-10MB

---

## Image Size Summary

| Stage | Dimensions | Format | Size |
|-------|------------|--------|------|
| Video capture | 1920×1080 (typical) | Raw pixels | N/A |
| Capture canvas | Same as video | RGBA | ~8MB (memory) |
| Stored image | Same as video | JPEG base64 | 150-400KB |
| OpenCV processing | Same as video | Grayscale | ~2MB (memory) |
| Warped card | 200×280 | RGBA | ~224KB (memory) |
| Hash input | 9×8 | RGBA | 288 bytes |
| Hash output | N/A | 8 bytes | 8 bytes |

---

## Data Flow Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                         SCAN BUTTON                              │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────┐
│  1. CAPTURE: video → canvas (full res) → JPEG base64           │
│     Output: imageDataUrl (~200KB), imageData (8MB temp)         │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────┐
│  2. DETECT: OpenCV edge detection → contour finding             │
│     Output: Array of 4-corner quadrilaterals                    │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼ (for each detected card)
┌─────────────────────────────────────────────────────────────────┐
│  3. WARP: Perspective transform quad → 200×280 rectangle        │
│     Output: Standardized card canvas                            │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────┐
│  4. TYPE: Analyze top-left icon region colors                   │
│     Output: Likely card types (thruster, reactor, etc.)         │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────┐
│  5. HASH: Resize 200×280 → 9×8 → grayscale → dHash              │
│     Output: 64-bit perceptual hash                              │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────┐
│  6. MATCH: Compare hash against 358-entry index                 │
│     Output: cardId, filename, side, confidence                  │
└─────────────────────────┬───────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────┐
│  7. STORE: Save to slot s1 (shift others down)                  │
│     Persisted to localStorage as base64 + metadata              │
└─────────────────────────────────────────────────────────────────┘
```

---

## Quadrilateral Comparison Notes

Quadrilaterals are **not compared to each other** for similarity. Each detected quad is processed independently:

1. Each quad is validated by its own properties:
   - Has 4 corners
   - Is convex
   - Meets area requirements
   - Has card-like aspect ratio

2. Size consistency filter compares areas to **median area** of all quads in frame:
   - Helps filter out false positives (e.g., small rectangles in card artwork)
   - If you scan 6 cards, all should be roughly same size

3. No duplicate detection:
   - If same card appears twice in frame, detected twice
   - No deduplication based on hash similarity

---

## Key Files Reference

| File | Purpose |
|------|---------|
| `ShowxatingShell.tsx` | Main capture orchestration |
| `visionPipeline.ts` | OpenCV detection + perspective warp |
| `cardMatcher.ts` | Hash computation + index matching |
| `showxatingStore.ts` | State management + localStorage |
| `card-index.json` | Pre-computed hashes (358 entries) |

---

## Response to Observations (2025-12-23 ~1:45am)

### Inline Note Responses

**#ross-note on JPEG quality (Step 1.4):**
> "we should try with higher quality"

Agreed. 85% was chosen to balance storage, but we're not storage-constrained (localStorage has 5-10MB, we use 1-3MB max). Higher quality helps OCR significantly.

**Recommendation:** Bump to 95% quality, or even use PNG for lossless. PNG might actually be smaller for card images with flat colors and sharp edges.

**#ross-note on convex requirement (Step 2.6.3):**
> "why must be convex?"

A real card viewed from any angle is always a convex quadrilateral. Non-convex shapes indicate:
- Detection error (merged two cards into one shape)
- Card artwork being detected instead of card edge
- Partially occluded card (finger covering corner)

Convexity is a cheap sanity check. That said, we could relax this for edge cases where lighting causes a slight concavity in the detected contour. Worth monitoring if we see valid cards rejected.

---

### Response to General Observations

**1. Should perspective warp happen BEFORE aspect ratio check?**

Current flow:
```
detect quad → check aspect ratio (on bounding box) → warp → hash
```

The aspect ratio check on bounding box is inaccurate for tilted cards. A card at 45° has a very different bounding box aspect ratio than the actual card.

However, warping is expensive (~5-10ms per card). Current approach filters out obvious non-cards before paying the warp cost.

**Recommendation:** Two-pass approach:
1. Keep loose aspect ratio filter (tolerance ±0.5) to reject obvious non-cards
2. After warp, verify true aspect ratio is ~1.4
3. Reject post-warp if aspect doesn't match

This catches cards at extreme angles while still filtering garbage early.

---

**2. Warp destination size (200×280) is too small**

You're right. 200×280 was chosen because:
- dHash only needs 9×8 (so anything larger is "enough")
- Smaller = faster warp

But this creates problems:
- OCR on 200×280 is marginal quality
- Type region becomes ~50×42 pixels (icon barely visible)
- Any future analysis is limited

**Recommendation:** Warp to **630×880 pixels** (10x actual card size in mm, maintains aspect ratio). This gives:
- Type region: ~157×132 pixels (plenty for color analysis)
- OCR region: Full-resolution type text
- Still fast enough (warp time scales linearly with output pixels)

For hash computation, we resize 630×880 → 9×8 anyway, so no accuracy loss.

---

**3. Color-based type matching is unreliable**

Agreed. In practice:
- Lighting varies wildly (warm, cool, shadows)
- Card printing varies
- Icon colors overlap (reactor red vs refinery orange)

The type detection currently provides a "boost" to matching but often boosts the wrong type.

**Recommendation:** Remove Phase 4 entirely. Replace with text extraction in Phase 4.5 (see below).

---

**4. No text matching in pipeline**

Correct. OCR currently only runs in `CorrectionModal` when manually correcting—it's not part of the automatic scan flow.

**Current state:**
- Scan → hash match only
- Manual correction → OCR available (but broken)

**What it should be:**
- Scan → hash match → OCR extraction → combined score

---

## Proposed Pipeline Refactor

### New Pipeline Flow

```
┌──────────────────────────────────────────────────────────────────┐
│  1. CAPTURE                                                       │
│     - JPEG 95% quality (or PNG)                                  │
│     - Full camera resolution                                      │
└─────────────────────────┬────────────────────────────────────────┘
                          │
                          ▼
┌──────────────────────────────────────────────────────────────────┐
│  2. DETECT (OpenCV)                                               │
│     - Find all quadrilaterals                                     │
│     - Loose aspect ratio filter (±0.5 tolerance)                 │
│     - Area filter (2-60% of frame)                               │
└─────────────────────────┬────────────────────────────────────────┘
                          │
                          ▼
┌──────────────────────────────────────────────────────────────────┐
│  3. WARP (per card)                                               │
│     - Output: 630×880 pixels (high-res)                          │
│     - Verify true aspect ratio post-warp                         │
└─────────────────────────┬────────────────────────────────────────┘
                          │
                          ├──────────────────┐
                          │                  │
                          ▼                  ▼
┌─────────────────────────────────┐  ┌─────────────────────────────┐
│  4a. HASH                        │  │  4b. OCR (parallel)          │
│  - Resize 630×880 → 9×8         │  │  - Extract type region       │
│  - Compute dHash                │  │  - OCR.space Engine 2        │
│  - Compare to index             │  │  - Returns: "Refinery", etc. │
└─────────────────────────┬───────┘  └──────────────┬──────────────┘
                          │                         │
                          └──────────┬──────────────┘
                                     │
                                     ▼
┌──────────────────────────────────────────────────────────────────┐
│  5. MATCH FUSION                                                  │
│     - Hash candidates (top 5 by Hamming distance)                │
│     - OCR text (card type)                                       │
│     - Boost candidates matching OCR type                         │
│     - Final ranking                                              │
└─────────────────────────┬────────────────────────────────────────┘
                          │
                          ▼
┌──────────────────────────────────────────────────────────────────┐
│  6. STORE                                                         │
│     - Full scan image (high quality)                             │
│     - Per-card: corners, hash, OCR text, match result            │
└──────────────────────────────────────────────────────────────────┘
```

### Key Changes

| Current | Proposed |
|---------|----------|
| JPEG 85% | JPEG 95% or PNG |
| Warp to 200×280 | Warp to 630×880 |
| Color-based type detection | Remove entirely |
| OCR only in correction modal | OCR in main pipeline |
| Hash-only matching | Hash + OCR fusion |

### Match Fusion Logic

```
For each detected card:
  1. Get hash matches (top 5 by distance)
  2. Get OCR result (type string)
  3. For each hash candidate:
     - If candidate.type === OCR.type: boost confidence +0.3
     - If candidate.type !== OCR.type: no change (don't penalize)
  4. Re-rank by boosted confidence
  5. Return top match
```

Why not penalize type mismatches? OCR might fail (return empty), and we don't want to hurt valid hash matches when OCR is unavailable.

### Implementation Priority

1. **Increase warp resolution** (630×880) - Easy, immediate benefit
2. **Increase JPEG quality** (95%) - Easy, improves OCR input
3. **Remove color-based type detection** - Simplifies code, removes noise
4. **Add OCR to pipeline** - Requires fixing OCR.space first
5. **Implement match fusion** - After OCR is reliable

### Files to Modify

| File | Changes |
|------|---------|
| `visionPipeline.ts` | Change warp output size to 630×880 |
| `ShowxatingShell.tsx` | Change JPEG quality to 0.95 |
| `cardMatcher.ts` | Remove `detectCardTypeFromCanvas()` calls |
| `cardMatcher.ts` | Add `matchWithOCR()` fusion function |
| `ShowxatingShell.tsx` | Add OCR call in capture flow |

---

## Open Questions

1. **OCR latency:** OCR.space takes 300-600ms. Run in parallel with hash computation? Or accept slightly slower scans?

2. **OCR fallback:** If OCR.space fails or returns empty, fall back to hash-only. But should we cache failed attempts to avoid re-trying?

3. **Card name OCR:** Currently extracting type ("Refinery"). Should we also extract card name ("Carbo-Chlorination") from bottom of card? More specific but harder to match.

4. **Index expansion:** Should card-index.json include type and name strings for text matching? Currently only has hash.
